{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e56e29b-6e7e-4781-b7fb-906d3e768580",
   "metadata": {},
   "source": [
    "# Project 3: Neural Net from Scratch\n",
    "\n",
    "In this project, you will create a neural network by hand to perform binary\n",
    "classification on a synthetic (made-up) dataset.  In this project, the data\n",
    "is a simple 2-dimensional dataset so it can be easily plotted and visualized.\n",
    "\n",
    "The goal of this project is not only to create the neural net from scratch, but\n",
    "also to get practice training neural nets of various depths and seeing how\n",
    "they work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3f631-4098-4914-ba1f-23148980f2c8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's examine our data set a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d094d-87d9-4ebc-a198-8a40dc7ecd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and plot\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "alldata = np.loadtxt(\"output.csv\", delimiter=\",\")\n",
    "X = data[:, :2]\n",
    "Y = data[:, 2:]\n",
    "\n",
    "# Number of data points\n",
    "m = 400\n",
    "\n",
    "print(\"Red points:\", np.sum(Y[:, 0] == 0))\n",
    "print(\"Blue points:\", np.sum(Y[:, 0] == 1))\n",
    "\n",
    "# Plot X, colored by Y\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], color='red', label='Class 0')\n",
    "plt.scatter(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], color='blue', label='Class 1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60c839-e90b-47a3-b200-14953a849918",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create testing and training sets.\n",
    "\n",
    "# In the real world, we would shuffle the data to make a random training/testing split,\n",
    "# but here, since we have 200 examples, we're going to use the first half of each\n",
    "# language for training and the last half for testing.\n",
    "\n",
    "# So create a training set of rows 0-99,\n",
    "# and a testing set of rows 100-199.\n",
    "                      \n",
    "X_train = None\n",
    "X_test = None\n",
    "Y_train = None\n",
    "Y_test = None\n",
    "\n",
    "# Sanity checks.\n",
    "print(X_train.shape)  # should be (200, 2)\n",
    "print(X_test.shape) # should be (200, 2)\n",
    "print(Y_train.shape) # should be (200, 1)\n",
    "print(Y_test.shape) # should be (200, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14251ca3-8d32-4d3d-ad6c-4a5e6f915c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network class.  The bulk of this project is writing a general-purpose\n",
    "# python class that will allow a neural network of any number of layers and\n",
    "# sizes of each layer.  You should fill in this class according to the backprop\n",
    "# handout provided.\n",
    "\n",
    "# The __init__ function and check_dimensions are already completed.\n",
    "# The others require you to fill in some code.\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "sigmoid = scipy.special.expit\n",
    "relu = lambda x: np.maximum(0, x)\n",
    "relu_deriv = np.vectorize(lambda x: 1 if x>0 else 0)\n",
    "from scipy.special import xlogy\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, sizes: list[int]):\n",
    "        \"\"\"\n",
    "        Initialize all of the variables we need to keep track of.\n",
    "        W, b, Z, A, delta, deriv_W, and deriv_b are lists of matrices.\n",
    "        \"\"\"\n",
    "        self.L = len(sizes)-1\n",
    "        self.sizes = sizes\n",
    "        self.W = [None] * (self.L+1)        # W[0] through W[L], but index 0 ignored\n",
    "        self.b = [None] * (self.L+1)        # b[0] through b[L], but index 0 ignored\n",
    "        self.Z = [None] * (self.L+1)        # Z[0] through b[L], but index 0 ignored\n",
    "        self.A = [None] * (self.L+1)        # A[0] through b[L], A[0] represents the input matrix X\n",
    "        self.delta = [None] * (self.L+1)    # index 0 ignored\n",
    "        self.deriv_W = [None] * (self.L+1)  # index 0 ignored\n",
    "        self.deriv_b = [None] * (self.L+1)  # index 0 ignored\n",
    "\n",
    "    def check_dimensions(self):\n",
    "        \"\"\"\n",
    "        Print out dimensions of all the matrices - useful for debugging.\n",
    "        \"\"\"\n",
    "        for ell in range(1, self.L+1):\n",
    "            if self.W[ell] is not None: print(f\"dim of W{ell} is\", self.W[ell].shape)\n",
    "            if self.b[ell] is not None: print(f\"dim of b{ell} is\", self.b[ell].shape) \n",
    "        for ell in range(1, self.L+1):\n",
    "            if self.Z[ell] is not None: print(f\"dim of Z{ell} is\", self.Z[ell].shape) \n",
    "            if self.A[ell] is not None: print(f\"dim of A{ell} is\", self.A[ell].shape) \n",
    "        for ell in range(1, self.L+1):\n",
    "            if self.delta[ell] is not None: print(f\"dim of delta{ell} is\", self.delta[ell].shape)\n",
    "            if self.deriv_W[ell] is not None: print(f\"dim of deriv_W{ell} is\", self.deriv_W[ell].shape) \n",
    "            if self.deriv_b[ell] is not None: print(f\"dim of deriv_b{ell} is\", self.deriv_b[ell].shape) \n",
    "\n",
    "    def init_weights_randomly(self):\n",
    "        \"\"\"\n",
    "        Set initial weights of W/b matrices randomly.\n",
    "        \"\"\"\n",
    "        np.random.seed(0)  # for reproducability\n",
    "        for ell in range(1, self.L+1):\n",
    "            W_rows = None\n",
    "            W_cols = None\n",
    "            b_length = None\n",
    "            self.W[ell] = np.random.normal(0, 1, (W_rows, W_cols))\n",
    "            self.b[ell] = np.random.normal(0, 1, (1, b_length))\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        X is a matrix of features, m rows by n cols.\n",
    "        returns: nothing\n",
    "        \n",
    "        This function should calculate and store all of the A and Z matrices appropriately.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward_prop(self, X, Y):\n",
    "        \"\"\"\n",
    "        X is a matrix of features, m rows by n cols.\n",
    "        Y is a matrix of correct outputs, m rows by n_L cols.\n",
    "           Because this is binary classification, Y is (m by 1), and each output is 0 or 1.\n",
    "        returns: nothing\n",
    "           \n",
    "        This function should calculate and store all of the delta, deriv_W, and deriv_B matrices appropriately.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        x is a vector of one input (one row from the X matrix).\n",
    "        returns: probability of x being in the \"1\" class (float between 0 and 1).\n",
    "\n",
    "        This function does the same thing as forward_prop, but for only one row of input,\n",
    "        and returns the result as a single float.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict_01(self, x):\n",
    "        \"\"\"\n",
    "        x is a vector of one input (one row from the X matrix).\n",
    "        returns: 0 or 1\n",
    "\n",
    "        This function does the same thing as predict, but rounds to 0 or 1.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def compute_cost(self, X, Y):\n",
    "        \"\"\"\n",
    "        X is a matrix of features, m rows by n cols.\n",
    "        Y is a matrix of correct outputs, m rows by n_L cols.\n",
    "           Because this is binary classification, Y is (m by 1), and each output is 0 or 1.\n",
    "\n",
    "        Return the total cost of putting all of the rows of X through the neural network, according\n",
    "        to the cross-entropy loss function.  You can do this by calling self.predict on each row of X,\n",
    "        or you can call self.forward_prop on X all at once and iterate over that.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373183b-50f7-4715-918e-03b11c8628cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single-layer neural network, 2 features going to 1 output\n",
    "nn1 = NeuralNet([2, 1])\n",
    "nn1.init_weights_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502f34a-8180-494e-88fa-ba76d62bc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for forward_prop and predict\n",
    "\n",
    "nn1.check_dimensions()\n",
    "nn1.init_weights_randomly()\n",
    "nn1.forward_prop(X_train[0:3])  # put first 3 rows of X through the NN\n",
    "print(nn1.Z[1])\n",
    "print()\n",
    "print(nn1.A[1])\n",
    "print()\n",
    "print(nn1.predict(X_train[0]))  # put first row through by itself, should match \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250a99f-c586-4e6e-a562-4b6728857800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for backprop.\n",
    "\n",
    "nn1.check_dimensions()\n",
    "nn1.init_weights_randomly()\n",
    "nn1.backward_prop(X_train[0:3], Y_train[0:3])   # put first 3 rows of X through backprop\n",
    "\n",
    "print(nn1.delta[1])\n",
    "print()\n",
    "print(nn1.deriv_W[1])\n",
    "print()\n",
    "print(nn1.deriv_b[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18092cc-5c8e-4ebc-84b7-03cef5ef7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for compute cost\n",
    "\n",
    "nn1.check_dimensions()\n",
    "nn1.init_weights_randomly()\n",
    "nn1.compute_cost(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bafc3b-775e-48f6-b503-86d567305319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write gradient descent:\n",
    "\n",
    "ALPHA = None\n",
    "\n",
    "J_sequence = []\n",
    "\n",
    "nn1.init_weights_randomly()\n",
    "\n",
    "# loop here\n",
    "\n",
    "print(\"Final cost:\", J_sequence[-1])\n",
    "print(\"Final parameters:\")\n",
    "print(nn1.W)\n",
    "print(nn1.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d536d2-b691-459a-a42f-200ca575475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529afca-0a10-41ee-aaed-79079e2df21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/train accuracy - write this function\n",
    "\n",
    "def compute_accuracy(X, Y):\n",
    "    \"\"\"\n",
    "    Returns fraction of the examples in X that were classified correctly.\n",
    "    You will want to call nn1.predict.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Testing and training accuracy for a 1-layer network should be close to 50%, but\n",
    "# mathematically for this neural net can't get much higher.\n",
    "\n",
    "print(compute_accuracy(X_train, Y_train))\n",
    "print(compute_accuracy(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d9397-df8a-459e-8639-52a58e508e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions on a graph.\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "# Create a grid of points\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(x_min, x_max, 500),\n",
    "    np.linspace(y_min, y_max, 500)\n",
    ")\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]  # shape: (resolution^2, 2)\n",
    "\n",
    "# Predict on the grid\n",
    "probs = np.array([nn1.predict_01(row) for row in grid])\n",
    "\n",
    "if probs.ndim > 1 and probs.shape[1] == 1:\n",
    "    probs = probs.ravel()\n",
    "Z = probs.reshape(xx.shape)\n",
    "\n",
    "# Plot contour\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(xx, yy, Z, levels=50, cmap=\"RdBu\", alpha=0.6)\n",
    "plt.colorbar(contour)\n",
    "\n",
    "# Plot original data points\n",
    "plt.scatter(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], color='red', label='Class 0')\n",
    "plt.scatter(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], color='blue', label='Class 1')\n",
    "plt.xticks(np.arange(-5, 5, 1))\n",
    "plt.yticks(np.arange(-5, 5, 1))\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc94572-4d75-43e0-b82f-3bbbdbf71948",
   "metadata": {},
   "source": [
    "# 2 layer neural net\n",
    "\n",
    "Now we will repeat this for a 2-layer network.  You get to pick the number\n",
    "of neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150065bb-9f1e-41bf-9b54-c63ea09b60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single-layer neural network, \n",
    "# 2 features going to (variable) hidden layer, going to 1 output\n",
    "nn2 = NeuralNet([2, 4, 1])   # good place to start\n",
    "nn2.init_weights_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104007c1-b8f7-440b-a190-95b177986d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2.check_dimensions()\n",
    "nn2.init_weights_randomly()\n",
    "nn2.forward_prop(X_train[0:3])  # put first 3 rows of X through the NN\n",
    "print(nn2.Z)\n",
    "print()\n",
    "print(nn2.A)\n",
    "print()\n",
    "print(nn2.predict(X_train[0]))  # put first row through by itself, should match \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d322e3-1258-491a-a9d8-7803d5af3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for backprop.\n",
    "\n",
    "nn2.check_dimensions()\n",
    "nn2.init_weights_randomly()\n",
    "nn2.backward_prop(X_train[0:3], Y_train[0:3])   # put first 3 rows of X through backprop\n",
    "\n",
    "print(nn2.delta)\n",
    "print()\n",
    "print(nn2.deriv_W)\n",
    "print()\n",
    "print(nn2.deriv_b)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2e2fa-fed1-48ca-826b-a57166468847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for compute cost\n",
    "\n",
    "nn2.check_dimensions()\n",
    "nn2.init_weights_randomly()\n",
    "nn2.compute_cost(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70884ee9-5408-445b-96c2-7a556dc0b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write gradient descent:\n",
    "\n",
    "ALPHA = None\n",
    "\n",
    "J_sequence = []\n",
    "\n",
    "nn2.init_weights_randomly()\n",
    "\n",
    "# write your gradient descent loop - don't forget to use \"nn2\" not \"nn1\"!\n",
    "\n",
    "print(\"Final cost:\", J_sequence[-1])\n",
    "print(\"Final parameters:\")\n",
    "print(nn2.W)\n",
    "print(nn2.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26477e-ef28-4227-9c4d-16e95ae8ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a20c3-1abf-4225-a7af-7952058d9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/train accuracy - write this function\n",
    "\n",
    "def compute_accuracy(X, Y):\n",
    "    \"\"\"\n",
    "    Returns fraction of the examples in X that were classified correctly.\n",
    "    You will want to call nn2.predict.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Testing and training accuracy for a 2-layer network can get easily over 90%.\n",
    "\n",
    "print(compute_accuracy(X_train, Y_train))\n",
    "print(compute_accuracy(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645de7fd-d976-4d92-a7ec-260867e1a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions on a graph.\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "# Create a grid of points\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(x_min, x_max, 500),\n",
    "    np.linspace(y_min, y_max, 500)\n",
    ")\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]  # shape: (resolution^2, 2)\n",
    "\n",
    "# Predict on the grid\n",
    "probs = np.array([nn2.predict_01(row) for row in grid])\n",
    "\n",
    "if probs.ndim > 1 and probs.shape[1] == 1:\n",
    "    probs = probs.ravel()\n",
    "Z = probs.reshape(xx.shape)\n",
    "\n",
    "# Plot contour\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(xx, yy, Z, levels=50, cmap=\"RdBu\", alpha=0.6)\n",
    "plt.colorbar(contour)\n",
    "\n",
    "# Plot original data points\n",
    "plt.scatter(X[Y[:, 0] == 0, 0], X[Y[:, 0] == 0, 1], color='red', label='Class 0')\n",
    "plt.scatter(X[Y[:, 0] == 1, 0], X[Y[:, 0] == 1, 1], color='blue', label='Class 1')\n",
    "plt.xticks(np.arange(-5, 5, 1))\n",
    "plt.yticks(np.arange(-5, 5, 1))\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a322ab-f889-4272-90e7-38b2291b1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the steps above with a 3-layer network, mostly just to show\n",
    "# your code works with 3 layers.  Keep the number of neurons at each\n",
    "# layer pretty small, try something like (2, 3, 3, 1).\n",
    "\n",
    "# Copy all the code from the cells above to create your 3-layer network.\n",
    "# Your final output must include gradient descent, the learning curve graph,\n",
    "# and the plot showing the \"shape\" of the predictions.\n",
    "\n",
    "# In the code that generates the shape of the predictions, you just need to find the one line\n",
    "# referencing nn1/nn2 and change it to your new variable (presumably nn3)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
